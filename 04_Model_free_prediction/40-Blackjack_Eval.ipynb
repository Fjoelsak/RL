{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lecture: Implementation of Monte Carlo Policy Evaluation for Blackjack\n",
    "\n",
    "We want to implement monte carlo policy evaluation for the `blackjack` environment provided by gymnasium.\n",
    "\n",
    "Blackjack is a card game in which the aim is to beat the dealer by getting cards that total closer to 21 (without totalling over 21) than the dealer's cards. The game begins with the dealer having one face-up and one face-down card, while the player has two face-up cards. All cards are drawn from an infinite deck (i.e. with replacements).\n",
    "\n",
    "- Face cards (Jack, Queen, King) have a point value of 10.\n",
    "- Aces can count either as 11 (a so-called \"usable ace\") or as 1.\n",
    "- Numerical cards (2-9) have a value corresponding to their number.\n",
    "\n",
    "The player sees the sum of the cards held. He can request more cards (hit) until he decides to stop (stick) or exceed 21 (bust, immediate loss).\n",
    "After the player has stopped, the dealer reveals his face-down card and draws cards until the total is 17 or more. If the dealer goes bust, the player wins.\n",
    "If neither the player nor the dealer busts, the result (win, loss, tie) is determined by whose total is closer to 21.\n",
    "To analyse different strategies, we use the gymnasium environment `Blackjack-v1`, the description of which can be found [here](https://gymnasium.farama.org/environments/toy_text/blackjack/)."
   ],
   "id": "681517df0fcfdc50"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/Fjoelsak/RL.git\n",
    "!cp RL/04_Mode_free_prediction/mc_eval_agent.py ./"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Excercises\n",
    "\n",
    "#### Task 1: Getting to know the environment\n",
    "\n",
    "Go to the [farama foundation documentation](https://gymnasium.farama.org/environments/toy_text/blackjack/) and determine how the state and action spaces are defined, how the reward function is implemented and how the condition for a termination of the episode is implemented. Which actions are coded here and how, and what information does an agent receive with the given definition of the observations?"
   ],
   "id": "4b17e437d3810adb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('Blackjack-v1')\n",
    "\n",
    "# Checking action and state space"
   ],
   "id": "36a6d254c22089f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 2\n",
    "\n",
    "Play ten time steps as an agent with randomised actions and look at the actions, observations and rewards for each time step.\n",
    "\tAlso display the end of each episode.\n",
    "\tTry to reproduce the individual games according to the rules mentioned above."
   ],
   "id": "2ce68fd5759e8f65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# YOUR CODE HERE",
   "id": "dcb4c13c47d49555"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 3\n",
    "\n",
    " Implement the policy presented in the lecture, in which you always draw a card as long as the sum of your cards is less than or equal to 19.\n",
    "\tTest your new policy with the setup defined in task 2 and follow the individual steps."
   ],
   "id": "b1961543095594f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def simple_policy(observation):\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ],
   "id": "1a08cc0f8f5602d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 4\n",
    "\n",
    "Now refactor your code. We want to have a separate class, an `MCEvalAgent`. Implement the function `gen_eps(env, policy)` in `mc_eval_agent.py` that runs through a single episode for the policy defined above and the current environment. While running through an episode, save all `states`, `actions` and `rewards` and return them as return values in the form of three lists. Test your refactored code by playing blackjack with 3 episodes and outputting the corresponding states, actions and rewards.\n",
    "\n",
    "Note: don't forget to reset the environment at the beginning of generating an episode."
   ],
   "id": "984862eb891cf012"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "from mc_eval_agent import MCEvalAgent\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\", render_mode=\"human\")\n",
    "\n",
    "n_eps = 3   # number of episodes\n",
    "\n",
    "agent = ...\n",
    "\n",
    "for _ in range(n_eps):\n",
    "    states, actions, rewards = agent.gen_eps(env, simple_policy)\n",
    "    print(\"States: \", states)\n",
    "    print(\"Actions: \", actions)\n",
    "    print(\"Rewards: \", rewards)\n",
    "    print(\"\")\n",
    "\n",
    "# Close the env\n",
    "env.close()"
   ],
   "id": "a73295bca68cb5b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 5\n",
    "\n",
    "Then implement the first-visit monte carlo policy evaluation algorithm in the function `eval(env, n_episodes, policy)`, which calculates the mean values of the values of the respective states for the specified number of episodes. $\\gamma$ shall be 0.9."
   ],
   "id": "fb0ed25f15c66029"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gymnasium as gym\n",
    "import time\n",
    "from mc_eval_agent import MCEvalAgent\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "\n",
    "agent = ...\n",
    "value = agent.eval(env, 500000, simple_policy)\n",
    "env.close()"
   ],
   "id": "f42d5df328e93029"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use the given plot function `plot_blackjack(V)` to visualise the figures from the lecture, i.e. the evaluation functions for n eps= 10,000 and 500,000.",
   "id": "4c21b45d24eae68a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "agent.plot_blackjack(value)",
   "id": "b19baa8becccb595"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 6\n",
    "\n",
    "We consider the following policy: if the sum of the player cards is greater than 18, we choose the action Stick with 80% probability and the action Hit with\n",
    "20% probability. If the sum of the player cards is less than or equal to 18, we choose the action Stick with 20% probability and the action Hit with 80% probability\n",
    ". What does the state value function look like?"
   ],
   "id": "947ec037a12269a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def stoch_policy(observation):\n",
    "    pass"
   ],
   "id": "888a1f4b2f913a85"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
