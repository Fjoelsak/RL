{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lecture: Implementation of Markov reward processes\n",
    "\n",
    "Suppose we have the markov reward process as depicted in the lecture slides. An implementation and sampling of episodes could look like in `markov_reward_process.py`. The implementation of the `markov_chain.py` is enhanced by rewards and a discount factor."
   ],
   "id": "bb44e97e37a73a2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/Fjoelsak/RL.git\n",
    "!cp RL/02_MDP/markov_reward_process.py ./"
   ],
   "id": "55bea5952467b60",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import markov_reward_process as mrp\n",
    "\n",
    "states = ['FB', 'C1', 'C2', 'C3', 'Pass', 'Pub', 'Sleep']\n",
    "rewards = [-1, -2, -2, -2, 10, 1, 0]\n",
    "transProbs = {0: {0: 0.9, 1: 0.1},\n",
    "            1: {0: 0.5, 2: 0.5},\n",
    "            2: {3: 0.8, 6: 0.2},\n",
    "            3: {4: 0.6, 5: 0.4},\n",
    "            4: {6: 1.0},\n",
    "            5: {1: 0.2, 2: 0.4, 3: 0.4},\n",
    "            6: {6: 1.0}\n",
    "}\n",
    "\n",
    "Student_MRP = mrp.MarkovRewardProcess(states, rewards, transProbs, [6], 0.9)\n",
    "print(\"Transposition probability matrix:\\n\", Student_MRP.trans_prob_matrix)\n",
    "\n",
    "for i in range(10):\n",
    "    eps, _ = Student_MRP.sample(1)\n",
    "    if len(eps) < 30:\n",
    "        print(\"Possible episode: \", eps)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Excercise: Calculating returns and analytical solution of the value function",
   "id": "2042d8a092e9bcf1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 1\n",
    "Implement the method `calc_return()` in `markov_reward_process.py` starting from a list of immediate rewards that the agent gets by sampling episodes in the MRP."
   ],
   "id": "ac8784d5e54477db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here, the calculations from the lecture slides are provided for testing purposes",
   "id": "ef3817a9476f8a10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(Student_MRP.calc_return([-2,-2,-2,10]))       # Result should be -2.25\n",
    "print(Student_MRP.calc_return([-2,-1,-1,-2,-2]))    # Result should be -3.125\n",
    "print(Student_MRP.calc_return([-2,-2,-2,1,-2,-2,10,0]))     # Result should be -3.41\n",
    "print(Student_MRP.calc_return([-2,-1,-1,-2,-2,-2, 1, -2]))    # Result should be -3.2"
   ],
   "id": "a19909106b4356ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Task 2\n",
    "\n",
    "Implement the `analytical_sol()` method in `markov_reward_process.py` with the approach described in the lecture slides for the given MRP by solving the linear equation. You can test your solution with the value functions shown for different discount factors in the lecture slides"
   ],
   "id": "559455bdefec4da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(Student_MRP.analytical_sol())",
   "id": "7f21a7cfb169955e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
