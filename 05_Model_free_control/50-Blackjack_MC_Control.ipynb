{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Lecture: Implementation of Monte Carlo On-Policy Control for Blackjack",
   "id": "feb04189129ae7fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Excercise",
   "id": "e277673172f71141"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 1\n",
    "\n",
    "Implement an equally distributed stochastic policy for the ``Blackjack-v1`` environment using the dimensions of the ``action_space`` and ``observation_space``."
   ],
   "id": "a0e705114d4f6108"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "\n",
    "policy = ...\n",
    "\n",
    "# easy testing\n",
    "assert policy.shape == (32, 11, 2, 2)\n",
    "assert policy[0,0,0,:].sum() == 1\n",
    "\n",
    "env.close()"
   ],
   "id": "4d008d151f5321b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 2\n",
    "Implement a function ``on_policy_mc_opt(env, $n_{eps}$, policy,\n",
    "gamma, eps)`` in the ``mc_control_agent.py`` file that implements the General Policy Improvement concept with Monte-Carlo policy evaluation and returns the state evaluation function $Q(s, a)$ and the policy $p$ as return values.\n",
    "\n",
    "*Note*: Your policy is now a stochastic policy $\\pi(a|s)$, i.e. for a given state the policy specifies probabilities for the possible actions. You may have to adjust your choice of action using the policy with respect to your Monte Carlo policy evaluation code from the lecture before."
   ],
   "id": "c0066bdd461c6115"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 2.1\n",
    "\n",
    "- Implement the ``gen_eps()`` method in the ``MCControlAgent`` class that generates a single episode given a policy. Mind the Note from above.\n"
   ],
   "id": "e512d5a0545b6ae7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from mc_control_agent_sol import MCControlAgent\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "\n",
    "agent = MCControlAgent(0.9)\n",
    "episode = agent.gen_eps(env, policy)\n",
    "\n",
    "print(\"States: \", [x[0] for x in episode])\n",
    "print(\"Actions: \", [x[1] for x in episode])\n",
    "print(\"Rewards: \", [x[2] for x in episode])\n"
   ],
   "id": "54e5bdc9f87a192f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 2.2\n",
    "\n",
    "- Implement the update step of the Q-value according to the first-visit MC concept in the ``on_policy_mc_opt(env, $n_{eps}$, policy, gamma, eps)`` method\n",
    "\n",
    "Note that you can get the return going recursively backwards through the episode and use the recursive return equation $G_{t} = R_t + \\gamma\\cdot G_{t+1}$\n",
    "- Implement an epsilon greedy policy by adjusting your policy every time you adjust the Q-values and reducing its epsilon for each episode according to\n",
    "\n",
    "    $$\\varepsilon_{k+1} = max (\\varepsilon_k \\cdot l, 0.01) $$\n",
    "\n",
    "    where $l$ is the adjustment rate of $\\varepsilon$. You can use the default value of $l = 0.99$ here."
   ],
   "id": "825ff9b22fd9c3fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from mc_control_agent_sol import MCControlAgent\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "\n",
    "policy = ...\n",
    "\n",
    "agent = MCControlAgent(0.9)\n",
    "Q, p = agent.on_policy_control(env, 500000, policy, eps_decay = 0.9)\n",
    "env.close()"
   ],
   "id": "8e9b64489e3d6f62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 2.3\n",
    "\n",
    "- Plot the policy as well as the state-value function using the two methods ``plot_policy()`` and ``plot_blackjack()``"
   ],
   "id": "d7bc4f30dfe7cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "agent.plot_policy(p)",
   "id": "84f80109202fb23d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, figsize=(4, 10),\n",
    "subplot_kw={'projection': '3d'})\n",
    "axes[0].set_title('value function without usable ace')\n",
    "axes[1].set_title('value function with usable ace')\n",
    "agent.plot_blackjack(env, p, Q, axes[0], axes[1])"
   ],
   "id": "b3fe2ea17e95a3d3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
