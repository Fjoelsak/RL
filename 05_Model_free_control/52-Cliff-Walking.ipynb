{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Lecture: SARSA and Q-learning for Cliff walking"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We look again at the `cliff walking` problem from the lecture and use the ``cliff-walking-v0`` environment provided by gymnasium. The problem is an episodic, non-discounted 4x12 gridworld that has a cliff in the bottom row. According to the figure, the agent receives one hundred minus points when crossing the cliff and the episode ends.\n",
    "The agent starts in the bottom left-hand corner (coordinates [0, 3]) and aims to reach the bottom right-hand corner (coordinates [11, 3]). In each time step, the agent gets a -1 reward.\n",
    "\n",
    "<img src=\"images/cliff-walk-theory.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Simulating the environment yields the following visualization from gymnasium.\n",
    "\n",
    "<img src=\"images/cliff-walk-gym.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Load the gymnasium environment and view the possible states, actions and their coding within the environment. What is the observation and action space? How are the actions encoded?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env  = gym.make('CliffWalking-v0', render_mode='human')\n",
    "\n",
    "observation, prob = env.reset()\n",
    "\n",
    "n_times = 20\n",
    "\n",
    "for i in range(n_times):\n",
    "\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    if terminated or truncated:\n",
    "        env.reset() \n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "With the given signature ``sarsa()`` of the template in ``cliff_walk_agent.py``, implement the on-policy SARSA algorithm to find the optimal path from the start to the end point. To do this, implement an epsilon greedy policy by reducing its epsilon for each episode according to\n",
    "\\begin{align*}\n",
    "\t\t\t\\varepsilon_{k+1} = \\max\\left( \\varepsilon_k \\cdot \\varepsilon_{\\text{decay}}, 0.01 \\right)\n",
    "\\end{align*}\n",
    "where $\\varepsilon_{\\text{decay}}$ is the adjustment rate of $\\varepsilon$. You can set $l=0.99$ as the default value here.\n",
    "\n",
    "Test your code with the ``run_episode()`` method. Note that within the run_episode method an $\\epsilon$-greedy policy is implemented! The default value for $\\epsilon$ is $0$ here."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from cliff_walk_agent import CliffWalkAgent\n",
    "import gymnasium as gym\n",
    "\n",
    "# initialize the agent\n",
    "agent = CliffWalkAgent(gamma = 1.0)\n",
    "\n",
    "# for control rendering should be deactivated due to performance reasons\n",
    "env  = gym.make('CliffWalking-v0')\n",
    "\n",
    "Q_sarsa, rewards = agent.sarsa(env, alpha=0.1,  epsilon=1, eps_decay=0.99, n_episodes=500)\n",
    "\n",
    "env  = gym.make('CliffWalking-v0', render_mode = 'human')\n",
    "\n",
    "rewards = agent.run_episode(env, Q_sarsa)\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Implement now Q-learning in the ``cliff_walk_agent.py``. Test the method as well with the ``run_episde()`` method.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from cliff_walk_agent import CliffWalkAgent\n",
    "import gymnasium as gym\n",
    "\n",
    "# initialize the agent\n",
    "agent = CliffWalkAgent(gamma = 1.0)\n",
    "\n",
    "# for control rendering should be deactivated due to performance reasons\n",
    "env  = gym.make('CliffWalking-v0')\n",
    "\n",
    "Q_qlearning, rewards = agent.qlearning(env, alpha=0.1,  epsilon=1, eps_decay=0.99, n_episodes=500)\n",
    "\n",
    "env  = gym.make('CliffWalking-v0', render_mode = 'human')\n",
    "\n",
    "rewards = agent.run_episode(env, Q_qlearning)\n",
    "\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "Run both algorithms with the following parameters ($\\alpha=0.1, \\varepsilon= 0.1, \\varepsilon_{\\text{decay}}=1., n_{\\text{episodes}}=500$) and visualise the respective rewards over the episodes.\tAverage the results over 100 evaluations and interpret the results, i.e. the averaged rewards over the 100 evaluations and the final trajectories of individual runs after 500 episodes of the Sarsa and the Q-learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "agent = CliffWalkAgent(gamma = 1.0)\n",
    "env  = gym.make('CliffWalking-v0')\n",
    "\n",
    "runs = 100\n",
    "avg_rew_sarsa = np.zeros(500)\n",
    "avg_rew_q = np.zeros(500)\n",
    "for go in range(runs):\n",
    "    Q_sarsa, reps_sarsa = agent.sarsa(env, alpha = 0.1, epsilon = .1, eps_decay=1., n_episodes=500)\n",
    "    Q_q, reps_q = agent.qlearning(env, alpha = 0.1, epsilon = .1, eps_decay=1., n_episodes=500)\n",
    "    #rewards = run_episode(env, Q)\n",
    "    avg_rew_sarsa += reps_sarsa \n",
    "    avg_rew_q += reps_q\n",
    "\n",
    "avg_rew_sarsa = np.array(avg_rew_sarsa)/runs\n",
    "avg_rew_q = np.array(avg_rew_q)/runs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.Figure()\n",
    "plt.plot(avg_rew_sarsa, label='Sarsa')\n",
    "plt.plot(avg_rew_q, label='Q-learning')\n",
    "plt.xlim(0,510)\n",
    "plt.ylim(-200, 0)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
