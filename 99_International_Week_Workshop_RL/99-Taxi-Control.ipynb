{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Workshop on Reinforcement Learning or How to drive a taxi in a data-driven way?\n",
    "\n",
    "Welcome to the workshop on Reinforcement Learning. We want to introduce the concept of Reinforcement Learning in a problem-based way with an interactive small example, the so called **Taxi environment**.\n",
    "\n",
    "There are four designated pick-up and drop-off locations (Red, Green, Yellow and Blue) in the 5x5 grid world. The taxi starts off at a random square and the passenger at one of the designated locations.\n",
    "\n",
    "The goal is move the taxi to the passenger’s location, pick up the passenger, move to the passenger’s desired destination, and drop off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "\n",
    "The player receives positive rewards for successfully dropping-off the passenger at the correct location. Negative rewards for incorrect attempts to pick-up/drop-off passenger and for each step where another reward is not received.\n",
    "\n",
    "<img src=\"mat/taxi.gif\" alt=\"Taxi driver randomly driving around\" width=\"400\"/>\n",
    "\n",
    "More information can be found in the [official documentation](https://gymnasium.farama.org/environments/toy_text/taxi/)"
   ],
   "id": "1398e6433bbc037d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Excercise 1: Visualizing your agent in the environment",
   "id": "b1251e9146130cd7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For using the taxi environment you need the `gymnasium` package.\n",
    "\n",
    "If you use python locally, you can use the `pygame` package to visualize the game. If you use colab instead, you can create a video from your agent acting in the environment.\n",
    "At first, we want to try out the environment by instantiating it and setup the typical RL data stream we introduced in the slides:\n",
    "\n",
    "<img src=\"mat/01-RL-datastream.png\" alt=\"RL datastream\" width=\"400\"/>\n",
    "\n",
    "Therefore, we implement a `while` loop, sample an **action** from the possible actions in the action space and **do** one step with action in the environment. As an agent, we get the next state (called **observation**, short obs), a **reward** and some additional information whether the episode has ended."
   ],
   "id": "ad5656668a0e7803"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Note: Use the following code if you use python locally",
   "id": "bce1cca305adeaf5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import pygame\n",
    "\n",
    "# instantiation of the environment\n",
    "env = gym.make('Taxi-v3', render_mode='human')\n",
    "# resetting the environment for first start\n",
    "obs, _ = env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # sample an action\n",
    "    action = env.action_space.sample()\n",
    "    # do one step in the environment\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    # flag whether the episode is finished\n",
    "    done = terminated or truncated\n",
    "    # render the game\n",
    "    env.render()\n",
    "\n",
    "    # this is just event handling that you can end the visualization by clicking q button\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.KEYDOWN:\n",
    "            if event.key == pygame.K_q:\n",
    "                pygame.quit()\n",
    "                done = True\n",
    "\n",
    "env.close()"
   ],
   "id": "d97ba13ff887398c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Note: Use the following code if you are using google colab",
   "id": "65137c7ddbe76f29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "import imageio\n",
    "\n",
    "# instantiation of the environment\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "# resetting the environment for first start\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# initialize a list of frames for video creation\n",
    "frames = []\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # capture the frame and append it to frames list\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "\n",
    "    # sample an action\n",
    "    action = env.action_space.sample()\n",
    "    # do one step in the environment\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # flag whether the episode is finished\n",
    "    done = terminated or truncated\n",
    "\n",
    "    # final rendering for last image of episode\n",
    "    if done:\n",
    "      frame = env.render()\n",
    "      frames.append(frame)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# save video as\n",
    "video_path = \"./taxi_vid.mp4\"\n",
    "imageio.mimsave(video_path, frames, fps=5)"
   ],
   "id": "55a102b46d7f139d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this is for displaying the video after saving\n",
    "mp4 = open(video_path, 'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=400 controls>\n",
    "    <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ],
   "id": "61887b6d1f53483",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Excercise 2\n",
    "\n",
    "Find out the dimensions of state and action space and check it with the ideas we introduced theortically before."
   ],
   "id": "422ca5d9fc23243e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(\"Dimension observation space: \", env.observation_space.n)",
   "id": "e755f084530da8b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(\"Dimension action space: \", env.action_space.n)",
   "id": "d744f5a551d2f8ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Excercise 3\n",
    "\n",
    "How would you build up and implement a strategy for the taxi driver to properly solve the taxi problem? Try out some thoughts and hardcode the optimal policy for a given problem instance. Look at the following situation which is defined as state 328:\n",
    "\n",
    "<img src=\"mat/taxi-seed328.png\" alt=\"Taxi problem state 328\" width=\"400\"/>\n",
    "\n",
    "- think about the exact order of actions you have to do\n",
    "- hardcode them in a list and try it out!\n",
    "\n",
    "Remark: The actions are encoded in the following way according to the documentation:\n",
    "\n",
    "- 0: Move south (down)\n",
    "- 1: Move north (up)\n",
    "- 2: Move east (right)\n",
    "- 3: Move west (left)\n",
    "- 4: Pickup passenger\n",
    "-5: Drop off passenger\n",
    "\n",
    "**Note**: from here on I will always provide the two options for visualizing either in local python setup or in colab"
   ],
   "id": "9c25897808f4b2e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Note: Use the following code if you use python locally",
   "id": "56de745b632dc95d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gymnasium as gym\n",
    "import pygame\n",
    "\n",
    "# instantiation of the environment\n",
    "env = gym.make('Taxi-v3', render_mode='human')\n",
    "# reseting the environment for first start\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# consider a specific problem instance\n",
    "env.unwrapped.s = 328\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # sample an action\n",
    "    action = env.action_space.sample()\n",
    "    # do one step in the environment\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    # flag whether the episode is finished\n",
    "    done = terminated or truncated\n",
    "    # render the game\n",
    "    env.render()\n",
    "\n",
    "    # this is just event handling that you can end the visualization by clicking q button\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.KEYDOWN:\n",
    "            if event.key == pygame.K_q:\n",
    "                pygame.quit()\n",
    "                done = True\n",
    "\n",
    "env.close()"
   ],
   "id": "5de293bd2cad68ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Note: Use the following code if you use google colab",
   "id": "253186a989875d60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "import imageio\n",
    "\n",
    "# instantiation of the environment\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "# resetting the environment for first start\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# consider a specific problem instance\n",
    "env.unwrapped.s = 328\n",
    "\n",
    "# initialize a list of frames for video creation\n",
    "frames = []\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # capture the frame and append it to frames list\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "\n",
    "    # sample an action\n",
    "    action = env.action_space.sample()\n",
    "    # do one step in the environment\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # flag whether the episode is finished\n",
    "    done = terminated or truncated\n",
    "\n",
    "    # final rendering for last image of episode\n",
    "    if done:\n",
    "      frame = env.render()\n",
    "      frames.append(frame)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# save video as\n",
    "video_path = \"./taxi_vid_own_policy.mp4\"\n",
    "imageio.mimsave(video_path, frames, fps=5)"
   ],
   "id": "70d9a3d904d323ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this is for displaying the video after saving\n",
    "mp4 = open(video_path, 'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=400 controls>\n",
    "    <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ],
   "id": "8b1f5f918cd325d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Excercise 4\n",
    "\n",
    "Let's start learning! Recall from the slides that there is an approach called Q-learning based on a Q-value function for each state-action pair. Doing updates wrt\n",
    "$$ Q(s,a) \\leftarrow Q(s,a) + \\alpha( r_{t+1} + \\gamma \\cdot \\max_{a} Q(s',a) - Q(s,a)) $$\n",
    "provides the optimal Q-function. Starting with a lot of exploration and estimating $Q(s,a)$ in each time step for the experience states and actions leads to the optimal function. Finally we can get the optimal policy by using the $\\text{arg} \\max_a Q(s,a)$ in each state.\n",
    "\n",
    "In order to balance exploration and exploitation during learning we introduced an $\\epsilon$-greedy approach. That is, we first try to explore a lot as a taxi driver in the environment and start exploiting the knowledge the more experience we have. One of the easiest implementations is a temporal decay of an $\\epsilon$ that starts with 1 and degrades with a factor $\\epsilon_{\\text{decay}} \\in [0,1)$. Hence, we have an $\\epsilon$-greedy policy as\n",
    "\n",
    "$$ \\pi(s) = \\begin{cases}\n",
    "        \\text{env.action\\_space.sample()} & \\text{if np.random.rand()} < \\epsilon \\\\\n",
    "        \\text{arg}\\max_a Q(s,a) & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "and $\\epsilon$ degrades in each time-step following\n",
    "$$ \\epsilon = \\epsilon \\cdot \\epsilon_{\\text{decay}} $$\n",
    "\n",
    "#### Task 1\n",
    "\n",
    "Implement the $\\epsilon$-greedy policy according to the formula above in the function `eps_greedy_policy(env, eps, state, Q)`. The function shall return the chosen action.\n"
   ],
   "id": "79bc9df0564da5bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def eps_greedy_policy(env, eps, s, Q):\n",
    "    \"\"\"\n",
    "    Selects an action using the epsilon-greedy strategy.\n",
    "\n",
    "    With probability `eps`, a random action is selected (exploration).\n",
    "    Otherwise, the action with the highest Q-value for the given state is chosen (exploitation).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment instance, used to sample random actions.\n",
    "    eps : float\n",
    "        The exploration rate (0 ≤ eps ≤ 1). Higher values increase the likelihood of random actions.\n",
    "    state : int\n",
    "        The current state of the agent (index into Q-table).\n",
    "    Q : np.ndarray\n",
    "        The Q-table with shape (num_states, num_actions), containing estimated action-values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    action : int\n",
    "        The selected action to take in the current state.\n",
    "    \"\"\"\n",
    "\n",
    "    # there is a method called np.argmax() for taking the argmax of a set of values\n",
    "    pass"
   ],
   "id": "ee7752aa0bc32138",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 2\n",
    "\n",
    "Implement the method `Qlearn(env, alpha, gamma, eps, eps_decay, max_eps)` that takes an environment without rendering (otherwise it will take some time) and several hyperparameters and learns and returns a Q-function for each state-action pair."
   ],
   "id": "d308bfb99981d831"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "def Qlearn(env, alpha, gamma, eps, eps_decay, max_eps):\n",
    "    \"\"\"\n",
    "    Performs Q-learning to learn an optimal Q-value table for a given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to learn from. Must have discrete observation and action spaces.\n",
    "    alpha : float\n",
    "        Learning rate (0 < alpha <= 1), determines how much new information overrides old.\n",
    "    gamma : float\n",
    "        Discount factor (0 <= gamma <= 1), determines the importance of future rewards.\n",
    "    eps : float\n",
    "        Initial epsilon for the epsilon-greedy policy (0 <= eps <= 1), controls exploration.\n",
    "    eps_decay : float\n",
    "        Multiplicative decay factor for epsilon after each episode (0 < eps_decay < 1).\n",
    "    max_eps : int\n",
    "        Number of episodes to train for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Q : numpy.ndarray\n",
    "        The learned Q-table of shape (num_states, num_actions), where each entry Q[s, a]\n",
    "        estimates the expected return of taking action a in state s and following the\n",
    "        learned policy thereafter.\n",
    "\n",
    "    \"\"\"\n",
    "    Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(max_eps):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            # sample an action\n",
    "            # TODO: select proper actions\n",
    "            a = ...\n",
    "            # do one step in the environment\n",
    "            s_prime, reward, terminated, truncated, _ = env.step(a)\n",
    "            total_reward += reward\n",
    "\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # update step of Q-learning\n",
    "            # TODO: do the proper update step\n",
    "            Q[s, a] = ...\n",
    "\n",
    "            # set state = next_state for next time step in episode\n",
    "            s = s_prime\n",
    "        rewards.append(total_reward)\n",
    "        eps = eps*eps_decay\n",
    "    return Q, rewards"
   ],
   "id": "8f16fbe3baddd0f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 3\n",
    "\n",
    "Test your implementation by running an episode with rendering. Check whether your taxi driver follows the optimal policy.\n",
    "\n"
   ],
   "id": "cb7f2d87205ad854"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# instantiation of the environment without rendering\n",
    "env = gym.make('Taxi-v3')\n",
    "\n",
    "# applying Q-learning on the env\n",
    "Q = Qlearn(env, alpha=0.1, gamma=0.99, eps=1, eps_decay=.99, max_eps=1000)\n",
    "\n",
    "# reseting the environment for first start\n",
    "env.close()"
   ],
   "id": "94afba530bf9142e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Note: Use the following code if you use python locally",
   "id": "2afe4fbf7cc0c4e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "import pygame\n",
    "\n",
    "# instantiation of the environment\n",
    "env = gym.make('Taxi-v3', render_mode='human')\n",
    "# reseting the environment for first start\n",
    "obs, _ = env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # sample an action\n",
    "    action = np.argmax(Q[obs, :])\n",
    "    # do one step in the environment\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    # flag whether the episode is finished\n",
    "    done = terminated or truncated\n",
    "    # render the game\n",
    "    env.render()\n",
    "\n",
    "    # this is just event handling that you can end the visualization by clicking q button\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.KEYDOWN:\n",
    "            if event.key == pygame.K_q:\n",
    "                pygame.quit()\n",
    "                done = True\n",
    "\n",
    "env.close()"
   ],
   "id": "17d65a5aca25abc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Note: Use the following code if you use google colab",
   "id": "ea7a68cf235fd04a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gymnasium as gym\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "import imageio\n",
    "\n",
    "# instantiation of the environment\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "# resetting the environment for first start\n",
    "obs, _ = env.reset()\n",
    "\n",
    "# initialize a list of frames for video creation\n",
    "frames = []\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # capture the frame and append it to frames list\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "\n",
    "    # sample an action\n",
    "    action = np.argmax(Q[obs, :])\n",
    "    # do one step in the environment\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # flag whether the episode is finished\n",
    "    done = terminated or truncated\n",
    "\n",
    "    # final rendering for last image of episode\n",
    "    if done:\n",
    "      frame = env.render()\n",
    "      frames.append(frame)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# save video as\n",
    "video_path = \"./taxi_vid_own_policy.mp4\"\n",
    "imageio.mimsave(video_path, frames, fps=5)"
   ],
   "id": "847bbd52d874aa64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this is for displaying the video after saving\n",
    "mp4 = open(video_path, 'rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "HTML(f\"\"\"\n",
    "<video width=400 controls>\n",
    "    <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ],
   "id": "eac270b3bc44ce51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Bonus Task:\n",
    "\n",
    "Try to find the best hyperparameter setting to learn as fast as you can the optimal policy. Tune the parameters to find the optimal solution with a minimum number of episodes. The best team gets awarded as **Q-Genius** team and gets a special reward!\n",
    "\n",
    "**Note**: it may help to gather and visualize the rewards over episodes over the whole training phase to check whether you really reached the optimal Q-function. Therefore, you need to track the total_reward for each episode in the `Qlearn()` method. Often the rewards as well as a moving average is visualized. An examplary code snippet may be found below."
   ],
   "id": "458a40ab5c018159"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# instantiation of the environment\n",
    "env = gym.make('Taxi-v3')\n",
    "Q, rew = Qlearn(env, alpha=0.1, gamma=0.99, eps=1, eps_decay=.99, max_eps=1000)\n",
    "# reseting the environment for first start\n",
    "env.close()\n",
    "\n",
    "# Compute moving average of rewards\n",
    "window = 50\n",
    "moving_avg = np.convolve(rew, np.ones(window)/window, mode='valid')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rew, label='Episode Reward', alpha=0.3)\n",
    "plt.plot(moving_avg, label=f'{window}-Episode Moving Average', linewidth=2)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Q-learning Performance on Taxi-v3')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "93d4cc08927c383f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
